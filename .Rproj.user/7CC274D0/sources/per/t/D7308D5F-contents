---
title: "ML_Assignment 5"
author: "Jacob Joy"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
library(factoextra)
library(dplyr)
library(dbscan)
library(fpc)
set.seed(123)
#laptop Data Import
#wines = read.csv("C:\\wine-clustering.csv")
#Desktop Data Import
wines = read.csv("F:\\wine-clustering.csv")
```

# Q1-A Explain how normalization and the number of clusters (k) can affect the clustering outcome?
When performing clustering the normalization of data as well as the number of clusters *k* is important in producing acurate outcomes. Clustering depends on the use of a distance metric commonly the euclidean distance, because of this if our data has different scales one variable could over take the others causing our model to not utilize all the relevant information. This leads to clusters that may not represent actual clusters and will only create clusters based on the highest scaled variable.
The number of clusters selected will also effect the outcome, accuracy, and meaningfulness of the results. If a small value of clusters *k* is used there may not be enough groups to sort the observations in a proper way, this can lead to lose of important patterns and clusters that should be separate will can end up being merged to fit the value of *k*. On the other hand if to high of a value of *k* is used there may be too many clusters to give any meaning fulling interpretations. A large number for *k* may also result in clusters that should not be split up being broken up into multiple clusters to satisfy our value for *k*. One final issue with a large value of *k* could mean clusters may be formed with too few observations making them not as meaningful. 


# Q1-B Code
```{r}
#Check the structure of the data
str(wines)

#summary check
summary(wines)

#Check for missing values
colMeans(is.na(wines))

#Normalize data
wines.norm = scale(wines)

#finding a optimal value for k using Elbow Method
fviz_nbclust(wines.norm, kmeans, method = "wss")

#finding optimal value for k using Silhouette
fviz_nbclust(wines.norm, kmeans, method = "silhouette")

#Performing K means with a k value of 3 based on the elbow point and the peak Silhoutte
k3 = kmeans(wines.norm, centers = 3, nstart = 25)

#Plot of clusters
fviz_cluster(k3, data = wines.norm)

#Centers
k3$centers

#size
k3$size

#withinss
k3$withinss

```

# Q1-B Explinations
Make sure to discuss the significance of the clusters formed and any insights you can draw from the results, i.e., interpret each cluster.

# Q2-A Explain how the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm differs from K-Means clustering, particularly in terms of handling noise and outliers.

DBSCAN and K-Means are different in how they handle clustering. DBSCAN is based on the density of data points and can perform well with clusters of varying size, DBSCAN clusters require a minimum amount of points and a distance between those points to form a cluster, this allows it to not be as sensitive to outliers and noise as those points can be left out. K-means requires the number of clusters specified before the algorithm is ran, this causes K-Means to make sure that every point in a dataset is put into a cluster even if that data point may be based on noise or is an outlier. K-Means is also based on centroids and the outliers can cause large shifts in what centroids are used. This makes K-Means much more sensitive to noise and outliers. Another main difference is DBSCAN can form clusters on non-linear data where k-means struggles because it is looking for globular groups. The differences between DBSCAN and K-Means can be overcome by having domain knowledge of the data and setting the parameters based on that knowledge.

# Q2-B Code
```{r}
#determining eps
dbscan::kNNdistplot(wines.norm, k = 3)
abline(h = 2.8, lty = 2)

dbs.wine = fpc::dbscan(wines.norm, eps = 2.8, MinPts = 3)
print(dbs.wine)
fviz_cluster(dbs.wine, wines.norm, stand = FALSE, frame = FALSE, geom = "point")
```

# Q2-B Explination
How do the parameters epsilon (Îµ) and minPts influence the clustering results?

# Q3 Explain pros and cons of each of these two methods based on the outputs observed in the previous questions.