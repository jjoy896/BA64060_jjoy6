---
title: "ML_Assignment 5"
author: "Jacob Joy"
output:
  html_document:
    df_print: paged
---

```{r}
library(factoextra)
library(dplyr)
library(dbscan)
library(fpc)
library(ggplot2)
set.seed(123)
#laptop Data Import
#wines = read.csv("C:\\wine-clustering.csv")
#Desktop Data Import
wines = read.csv("F:\\wine-clustering.csv")
```

# Q1-A Explain how normalization and the number of clusters (k) can affect the clustering outcome?
When performing clustering the normalization of data as well as the number of clusters *k* is important in producing acurate outcomes. Clustering depends on the use of a distance metric commonly the euclidean distance, because of this if our data has different scales one variable could over take the others causing our model to not utilize all the relevant information. This leads to clusters that may not represent actual clusters and will be over taken by the largest valued variables.
The number of clusters selected will also effect the outcome, accuracy, and meaningfulness of the results. If a small value of clusters *k* is used there may not be enough groups to sort the observations in a proper way, this can lead to lose of important patterns and clusters that should be separate will can end up being merged to fit the value of *k*. On the other hand if to high of a value of *k* is used there may be too many clusters to give any meaningful interpretations. A large number for *k* may also result in clusters that should not be split up being broken up into multiple clusters to satisfy our value for *k*. One final issue with a large value of *k* could mean clusters may be formed with too few observations making them not as meaningful. 


# Q1-B Code
```{r}
#Check the structure of the data
str(wines)

#summary check
summary(wines)

#Check for missing values
colMeans(is.na(wines))

#Normalize data
wines.norm = scale(wines)

#finding a optimal value for k using Elbow Method
fviz_nbclust(wines.norm, kmeans, method = "wss")

#finding optimal value for k using Silhouette
fviz_nbclust(wines.norm, kmeans, method = "silhouette")

#Performing K means with a k value of 3 based on the elbow point and the peak Silhouette
k3 = kmeans(wines.norm, centers = 3, nstart = 25)

#Plot of clusters
fviz_cluster(k3, data = wines.norm)

#Centers
k3$centers

#Setting up and plotting centroids
mincenter = min(k3$centers)
maxcenter = max(k3$centers)
plot(c(0), xaxt = 'n', ylab = "", type ="l", ylim = c(mincenter, maxcenter), xlim = c(0, 13))
axis(1, at = c(1:13), xpd = TRUE, labels = c("Alc","Malic","Ash","Alca_Ash","Mag","T_Phenols","Flavanoids","NonFlav", "Proanth", "Color", "Hue", "OD280", "Proline"), cex.axis = 0.4)

for(i in c(1:3))
  lines(k3$centers[i,], lty = i, lwd = 2)

text(x = 0.5, y = k3$centers[, 1], labels = paste("Cluster", c(1:3)))

#size
k3$size

#withinss
k3$withinss
```

# Q1-B Explinations
Make sure to discuss the significance of the clusters formed and any insights you can draw from the results, i.e., interpret each cluster.

Based on wss and silhouette it was determined that the value of *k* should be 3. After running kmeans clustering 3 distinct clusters were plotted, because our data has more than 2 dimensions the plot uses Principle Component Analysis to visualize our clusters. In order to better interpret the clusters a plot of the centroids was created. Cluster one has wines that have the highest malic acid, alcanlinity of ash, nonflavanoid phenolic, and color intensity. Cluster one is lowest in Flavanoids, Proanthocyanins, Hue, and OD280. Cluster 2 is highest in Alchohol, Ash, Flavanoids, Proanthocyanins, OD280, and Proline. Cluster 2 looks to be about equal in Hue to cluster 3. Cluster 2 is lowest in Alcalinity of Ash, nonflavanoid Phenols. Cluster 3 is not highest in any feature and is tied with Cluster 2 in Hue. Cluster 3 is lowest in Alcohol, Malic Acid, Ash, Magnesium, Color Intensity, and Proline.
Based on the cluster plot it appears that the most variability is found in Cluster 1, however our total overall variability is low for this data at only 55.4% Cluster 3 has the highest withinss value also indicating that the density in cluster 3 is lower and more spread out then the other clusters. 


# Q2-A 
Explain how the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm differs from K-Means clustering, particularly in terms of handling noise and outliers.

DBSCAN and K-Means are different in how they handle clustering. DBSCAN is based on the density of data points and can perform well with clusters of varying size, DBSCAN clusters require a minimum amount of points and a distance between those points to form a cluster, this allows it to not be as sensitive to outliers and noise as those points can be left out. K-means requires the number of clusters specified before the algorithm is ran, this causes K-Means to make sure that every point in a dataset is put into a cluster even if that data point may be based on noise or is an outlier. K-Means is also based on centroids and the outliers can cause large shifts in what centroids are used. This makes K-Means much more sensitive to noise and outliers. Another main difference is DBSCAN can form clusters on non-linear data where k-means struggles because it is looking for globular groups. The differences between DBSCAN and K-Means can be overcome by having domain knowledge of the data and setting the parameters based on that knowledge.

# Q2-B Code as requested
```{r}
#determining eps
dbscan::kNNdistplot(wines.norm, k = 3)
abline(h = 2.8, lty = 2)

dbs.wine = fpc::dbscan(wines.norm, eps = 2.8, MinPts = 3)
print(dbs.wine)
fviz_cluster(dbs.wine, wines.norm, stand = FALSE, frame = FALSE, geom = "point")
```
  
# Using larger value of k based on rules of thumb
```{r}
#determining eps
dbscan::kNNdistplot(wines.norm, k = 26)
abline(h = 3.7, lty = 2)

dbs.wine = fpc::dbscan(wines.norm, eps = 3.7, MinPts = 26)
print(dbs.wine)
fviz_cluster(dbs.wine, wines.norm, stand = FALSE, frame = FALSE, geom = "point")
```
  
# Testing of Multiple values for epsilon
```{r}
#determining eps
dbscan::kNNdistplot(wines.norm, k = 3)
abline(h = 2.3, lty = 2)

dbs.wine = fpc::dbscan(wines.norm, eps = 2.3, MinPts = 3)
print(dbs.wine)
fviz_cluster(dbs.wine, wines.norm, stand = FALSE, frame = FALSE, geom = "point")
```

# Q2-B Explination
How do the parameters epsilon (Îµ) and minPts influence the clustering results?
After plotting the distance values of KNN using the value of 3 for K it appears that the distance changes sharpley at around 2.8 which is the value I used for epsilon. I used 3 for minPts as that is the value used for the number of clusters as requested in the problem and what was used to determine epsilon. Using these values it appears there is one cluster when using DBSCAN and 15 points of noise. Redoing DBSCAN using a value for minPts based on dimensions + 1 a new value for epsilon was calculated to be 3.1. This did not change the cluster much, but did reduce the number of noise points to 10. One more try was conducted using the value of 26 for minPts based on another rule of thumb that uses 2*dimensions. This still created the same cluster and reduced the noise points by 2. This set of tests indicates that if epsilon and minPts move together in a set way the results are similar. However, if I just change the value of minPts to the value found by 2xDimensions I find that 2 clusters are formed and there is an increase in the number of noise points identified. When changing the value of epsilon to a larger value 4.5 all points ended up being in 1 cluster with no noise. Using a small value of epsilon 0.5 all points are identified as noise. Changing epsilon slightly to 3.3 does discover another cluster but it is overlapping cluster 1. This is not accurate because there should be distinct types of wine. At a value of 2.3 for epsilon we find 3 clusters however cluster 2 only has 3 observations which is our exact minimum. At this value of epsilon our clusters do start to resemble what was found in K-Means. The issue is that our data may have to many features that are not relevant to this classification and that the data is inconsistent in its density which is why we have overlapping clusters and in some combinations of parameters only 1 cluster. 

# Q3 Explain pros and cons of each of these two methods based on the outputs observed in the previous questions.
**Kmeans**    
**Pros**    
The pros of Kmeans for our data set are it gives 3 distinct clusters and appears to work better with the lower amount of observations that this data set contains.  
**Cons**    
The cons of Kmeans for our data set are the total variability is low and the clusters are all similar in size meaning we may be missing out on some detail and there may be more clusters then 3.    
**DBSCAN**    
**Pro**    
The pro of DBSCAN in this case is we do not need to worry about outliers and noise as the noise is identified.    
**Cons**    
The cons of DBSCAN are we do not have enough observations and to many features that DBSCAN can only determine 1 cluster and some noise points, which does not tell us anything. Another con is that when determining the value for epsilon the kNN distance curve is not very smooth indicating multiple levels of density causing DBSCAN to under perform.    

Based on the results of the two methods I would work to improve Kmeans by utilizing dimension reduction as in this case kmeans appears to already present more meaningful patterns and interpretations. In order to make DBSCAN more useful in this case we would need more data as well as reducing its dimensions. 