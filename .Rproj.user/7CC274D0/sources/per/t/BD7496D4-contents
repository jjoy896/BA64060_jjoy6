---
title: "ML_Assignment_4"
author: "Jacob Joy"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# Setup
```{r}
#call Packages
library(naivebayes)
library(caret)
library(pivottabler)
library(gmodels)

#Import Data
heart_Data = read.csv("C:\\Users\\Urza\\OneDrive\\BA 64060 Fundementals of Machine Learning\\Module 5\\Assignment 4\\heart_disease.csv")

#Set Seed for consistency
set.seed(123)

#Creating Dummy variable for Target Yes if MAX HeartRate 
#greater than 170 else NO
for(i in 1:nrow(heart_Data)){
  heart_Data$Target[i] = ifelse(heart_Data$MAX_HeartRate[i] > 170, "YES", "NO")
}

#Creating Dummy variable for BP_New if Blood_Pressure is greater 
#than 120 else NO
for(i in 1:nrow(heart_Data)){
  heart_Data$BP_New[i] = ifelse(heart_Data$Blood_Pressure[i] > 120, "YES", "NO")
}
```
# Setup Information
I preformed the above setup, which included created dummy variables for two numerical variables for use in the Naive Bayes Classifier because continuous numeric variables do not work well with this model. 

# Question 1
```{r}
#Created a table to compare the target for heart disease with if they had 
#chest pain
#Target_table = table(heart_Data$Target, heart_Data$chest_pain_type, 
                   #  dnn = c("Heart Disease", "Chest Pain"))

Target_table <- table(heart_Data$Target) 
Target_table
Probability_No <- Target_table["NO"] / sum(Target_table)
Probability_No
#Created the table to display probabilities instead of just frequency count
#prop.table(Target_table)
```
# Question 1 Explanation
Based on only the information on the table created if a person presented with Chest Pain the most likely outcome would be that they do not have Heart Disease. However, because chest pain can be life threatening a doctor would still want to verify on more then the symptom of Chest Pain.

# Question 2 A Code
```{r}
#Create a new dataframe with the first 30 entries which only includes the 
#features Target, BP_New, and chest_pain_type
heart_data30 = heart_Data[1:30, c("Target", "BP_New", "chest_pain_type")]

#Creation of piviot table using all three variables
pivot1 = ftable(heart_data30)
pivot1

#Creation of each probability
P_YES_YES_1 = nrow(subset(heart_data30, Target == "YES" & BP_New == "YES" 
                          & chest_pain_type == 1)) / nrow(heart_data30)

P_YES_NO_0 = nrow(subset(heart_data30, Target == "YES" & BP_New == "NO" & 
                           chest_pain_type == 0)) / nrow(heart_data30)

P_YES_NO_1 = nrow(subset(heart_data30, Target == "YES" & BP_New == "NO" & 
                           chest_pain_type == 1)) / nrow(heart_data30)

P_YES_YES_0 = nrow(subset(heart_data30, Target == "YES" & BP_New == "YES" & 
                            chest_pain_type == 0)) / nrow(heart_data30)

P_YES_1 = nrow(subset(heart_data30, BP_New == "YES" & 
                        chest_pain_type == 1)) / nrow(heart_data30)

P_NO_0 = nrow(subset(heart_data30, BP_New == "NO" & 
                       chest_pain_type == 0)) / nrow(heart_data30)

P_NO_1 = nrow(subset(heart_data30, BP_New == "NO" & 
                       chest_pain_type == 1)) / nrow(heart_data30)

P_YES_0 = nrow(subset(heart_data30, BP_New == "YES" & 
                        chest_pain_type == 0)) / nrow(heart_data30)

#Creation of the variables that have the conditional probabilities 
#with the outcome being yes for target.

P11 = P_YES_YES_0 / P_YES_0
P22 = P_YES_YES_1 / P_YES_1
P33 = P_YES_NO_0 / P_NO_0
P44 = P_YES_NO_1 / P_NO_1

print(paste("The conditional probability of having heart disease with high BP and No chest Pain", P11))

print(paste("The conditional probability of having heart disease with high BP and chest Pain", P22))

print(paste("The conditional probability of having heart disease withregular BP and No chest Pain", P33))

print(paste("The conditional probability of having heart disease with high BP and chest Pain", P44))


Object1 <- ftable(heart_data30)  
Object2 <- ftable(heart_data30[,-1])
P1 = Object1[4,1] / Object2[2,1]
P2 = Object1[4,2] / Object2[2,2]
P3 = Object1[3,1] / Object2[1,1]
P4 = Object1[3,2] / Object2[1,2]

P1
P2
P3
P4
```
# Question 2 A Explination
In question 2 A I created a new dataframe of only the first 30 observations and then created a pivot table based on this information. Next, I created the probabilities for each outcome of variables but only if the Target variable was Yes. Using the created probabilities I used the formula for conditional probability to find the 4 different variations that could occur. Based on just these probabilities the "only" indication of heart disease is having chest pain.

# Question 2 B Code
```{r}
#Created a numeric variable of 30 entries all set to 0
Probability_Target = rep(0,30)

#Using a for loop and an if statement each observation in heart_data30 was 
#checked and given a probability based on the conditional probabilities 
#calculated in Part A, because the only variable that resulted in heart_disease 
#was the presence of chest pain any other option would be set to 0

for (i in 1:30){
  if (heart_data30$BP_New[i] == "YES" & heart_data30$chest_pain_type[i] == 1){
    Probability_Target[i] = P2
    } else if (heart_data30$BP_New[i] == "NO" & 
               heart_data30$chest_pain_type[i] == 1){
      
      Probability_Target[i] = P4
    } else {
      Probability_Target[i] = 0
    }
}
#Adding the Probability_Target variable to our dataframe
heart_data30$Probability_Target = Probability_Target

#Using the Probability_Target and a cutoff value of 0.5 a prediction was 
#made if the person would have heart disease or not

heart_data30$Pred_Probability = 
  ifelse(heart_data30$Probability_Target > 0.5, "Yes", "No")

table(heart_data30$Target, heart_data30$Pred_Probability)
```
# Question 2 B Explination
After completing Step B and creating a table to see the results from our target compared with the Predicted classification we can see that with a cutoff value of 0.5 there would have been a potential to miss 2 people that had heart disease. Again because of the nature of this data and peoples lives the cutoff value would want to be lowered drastically to make sure that people with chest pain are evaluated. (This is true in the real world, as your doctor will send you to the ER if you have chest pain)

# Question 2 C
```{r}
#Created the probability of having heart disease
P_YES = nrow(subset(heart_data30, Target == "YES")) / nrow(heart_data30)

P2_YES =nrow(subset(heart_data30, Target == "YES")) / nrow(heart_data30)
P2_YES_YES_1 = nrow(subset(heart_data30, Target == "YES" & BP_New == "YES" & chest_pain_type == 1)) / nrow(heart_data30)
P2_YES_1 = nrow(subset(heart_data30, BP_New == "YES" & chest_pain_type == 1)) / nrow(heart_data30)
#Using the case of a observation having high BP and Chest Pain, 
#I used Bayes theorem to find the probability
naive = (P2_YES_YES_1 * P2_YES) / P2_YES_1
naive

P_BPY_TY = nrow(subset(heart_data30, Target == "YES" & BP_New == "YES")) / nrow(subset(heart_data30, Target == "YES"))
P_CP1_TY = nrow(subset(heart_data30, Target == "YES" & chest_pain_type == 1)) / nrow(subset(heart_data30, Target == "YES"))

Naive2 = (P_BPY_TY * P_CP1_TY * P_YES)/ P2_YES_1
Naive2
```
# Question 2 C Explination
The probability found using Bayes Theorem of a person having heart disease who has high BP and Chest pain is 1.67% This makes sense based on the data we have and problems conducted before this. The chances of heart disease is low over all and when looking at only 2 specific events  we see that this is even lower. This also conforms to the naive case where in a non emergency event patients health data could be looked at quickly to determine the likely hood of heart disease.

# Question 2 C Redo
```{r}

```

# Question 3 Code
```{r}
#Partitioned the data 
train_index = createDataPartition(heart_Data$Target, p = 0.6, list = FALSE)

#Training data created
trainDF = heart_Data[train_index,]

#Validation data created
validationDF = heart_Data[-train_index,]

#Creation of the Naive Bayes Classifier based on BP and Chest Pain
nb_model = naive_bayes(Target ~ BP_New + chest_pain_type, 
                       data = trainDF, laplace = 0)

#Predictions made using the Classifier
train_pred = predict(nb_model, validationDF)

#confusion matrix
validationDF$Target = as.factor(validationDF$Target)
confusionMatrix(train_pred, validationDF$Target, positive = "YES")

```
# Question 3 Explination
After partitioning our data and training a naive Bayes classifier a confusion matrix was created which shows that we have many misclassifications. However, our sensitivity is 1 and that is good in this case because we do not want to miss anyone that has heart disease. After multiple runs the sensitivity has stayed at 1. I also tested a laplace of 1 to see if this would change any of the results due to some conditions that would have a probability of 0. However, the results were similar and the sensitivity stayed at 1.
