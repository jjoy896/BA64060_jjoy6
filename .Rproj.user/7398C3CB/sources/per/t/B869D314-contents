---
title: "Final Project Report"
author: "Jacob Joy"
output:
  pdf_document: default
  html_notebook: default
---
```{r include=FALSE}
library(dplyr)
library(ggplot2)
library(caret)
library(corrplot)
library(factoextra)
library(dbscan)
library(fpc)
library(cluster)
library(class)
library(gmodels)
library(naivebayes)
library(ggcorrplot)

#Laptop Data import
fuelDF = read.csv("C:\\EIA923.csv")

#Desktop Data Import
#fuelDF = read.csv("F:\\EIA923.csv")


#seed set according to last 4 of student number
set.seed(7165)

#Look at structure of data
str(fuelDF)

#Removing plant id and label
fuelDF_CH = fuelDF %>% select(-plant_id_eia, -plant_id_eia_label)

#Quick look at data summary
summary(fuelDF_CH)

#Checking for missing values
colMeans(is.na(fuelDF_CH))

#Based on str needed to change type char to factor
fuelDF_CH$fuel_type_code_pudl = as.factor(fuelDF_CH$fuel_type_code_pudl)

#Creation of dummy variables for fuel type which is a categorical variable
dummy_fuel = dummyVars(~fuel_type_code_pudl, data = fuelDF_CH)
dummy_matrix = predict(dummy_fuel, fuelDF_CH)
fuelDF_CH.dummy = cbind(dummy_matrix, fuelDF_CH)

#looking at each variable and potential outliers 
boxplot(fuelDF$fuel_received_units, main = "Fuel Recieved Units")
boxplot(fuelDF$sulfur_content_pct)
boxplot(fuelDF$fuel_mmbtu_per_unit)
boxplot(fuelDF$ash_content_pct)
boxplot(fuelDF$fuel_cost_per_mmbtu)

#additional look at sulfur content values
hist(fuelDF$sulfur_content_pct)

#This code was used to look at the fuel_received_units for each type of energy source
#Discussed in report not used in final code

#gasDF = fuelDF %>% filter(fuel_type_code_pudl == 'gas')
#oilDF = fuelDF %>% filter(fuel_type_code_pudl == 'oil')
#coalDF = fuelDF %>% filter(fuel_type_code_pudl == 'coal')
#boxplot(gasDF$fuel_received_units)
#summary(gasDF)
#summary(oilDF)
#summary(coalDF)

#removing the categorical variable fuel_type_code_pudl, redundant with dummy Vars created
#normalized the data frame
fuelDF.clean = fuelDF_CH.dummy %>% select(-fuel_type_code_pudl) %>% scale()

#Correlation matrix to determine features to use
cor_matrix = data.frame(cor(fuelDF.clean))
ggcorrplot(cor_matrix)

#removal of fuel_received_units
fuelDF.clean = as.data.frame(fuelDF.clean)
fuelDF.clean = fuelDF.clean %>% select(-fuel_received_units)

#determining a value for k
fviz_nbclust(fuelDF.clean, kmeans, method = "wss")
fviz_nbclust(fuelDF.clean, kmeans, method = "silhouette")

kclusters = kmeans(fuelDF.clean, centers = 3)
#kclusters = kcca(fuelDF.clean, k=3, kccaFamily("jaccard"))

#ja = predict(kclusters)
fviz_cluster(kclusters, data = fuelDF.clean)

#Centers
kclusters$centers

#Size
kclusters$size

fuelDF %>% group_by(fuel_type_code_pudl) %>% summarize(count = n())

#Setting up and plotting centroids
mincenter = min(kclusters$centers)
maxcenter = max(kclusters$centers)
plot(c(0), xaxt = 'n', ylab = "", type ="l", ylim = c(mincenter, maxcenter), xlim = c(0, 7))
axis(1, at = c(1:7), xpd = TRUE, labels = c("Coal","Gas","Oil","MMBTU", "Sulfur", "Ash", "Cost"), cex.axis = 1)


lines(kclusters$centers[1,], lty = 1, lwd = 2, col = "red")
lines(kclusters$centers[2,], lty = 2, lwd = 2, col = "green")
lines(kclusters$centers[3,], lty = 3, lwd = 2, col = "blue")

legend(x = "topleft", box.col = "brown", 
       bg ="yellow", box.lwd = 2 , title="Clusters",  
       legend=c("Cluster 1", "Cluster 2", "Cluster 3"),  
       fill = c("red","green", "blue")) 

#determining eps
dbscan::kNNdistplot(fuelDF.clean, k = 5)
abline(h = 0.7, lty = 2)

dbs = fpc::dbscan(fuelDF.clean, eps = 0.7, MinPts = 5)
print(dbs)

fviz_cluster(dbs, fuelDF.clean, stand = FALSE, frame = FALSE, geom = "point")

#performing agnes using each different methods
agnes_single = agnes(fuelDF.clean, method = "single")
agnes_complete = agnes(fuelDF.clean, method = "complete")
agnes_average = agnes(fuelDF.clean, method = "average")
agnes_ward = agnes(fuelDF.clean, method = "ward")

#Printing the AC of each method to determine the best one to select for graphing and using
print(agnes_single$ac)
print(agnes_complete$ac)
print(agnes_average$ac)
print(agnes_ward$ac)

#Dendogram of Agnes that used the Ward method
pltree(agnes_ward, cex = 0.6, hang = -1, main = "Dendrogram of Agnes")
rect.hclust(agnes_ward, k = 3, border = 1:3)

#Using DIANA
diana.hc = diana(fuelDF.clean)

#Prinitng DIANAs DC
print(diana.hc$dc)

#Dendogram of Diana
#pltree(diana.hc, cex = 0.6, hang = -1, main = "Dendogram of Diana")
#rect.hclust(diana.hc, k = 3, border = 1:3)

#Plot of AGNES Clusters
agnes.grp = cutree(agnes_ward, k = 3)
fviz_cluster(list(data = fuelDF.clean, cluster = agnes.grp))

#Partitioned the data in half and stratified based on the target variable
train_index = createDataPartition(fuelDF_CH$fuel_type_code_pudl, p = 0.7, list = FALSE)

#Half of the data will be used for training
trainDF = fuelDF_CH[train_index,]

#The other half will be further partitioned into validation and testing
tempdf = fuelDF_CH[-train_index,]

#Partitioned the remaining data into validation index of 20% stratified based on the target variable to stay consistent
validation_index = createDataPartition(tempdf$fuel_type_code_pudl, p = 0.2, list = FALSE)

#Created the validation data frame
validationDF = tempdf[validation_index,]

#Created the testDF using the left over data
testDF = tempdf[-validation_index,]

#Removed the temp data frame 
rm(tempdf)

#created normalized values using z-score normalization.
norm_values = preProcess(trainDF[,3:6], method= c("center","scale"))

#Based on the normalized values from the training set testing, validation, and training normalized data frames are created.
testDF_norm = predict(norm_values, testDF[,(3:6)])
validationDF_norm = predict(norm_values, validationDF[,3:6])
trainDF_norm = predict(norm_values, trainDF[,3:6])

train_labels = trainDF[,1]
test_labels = testDF[,1]
validation_labels = validationDF[,1]

#Created a data frame that contains values of k from 1 - 15
accuracyDF = data.frame(k = seq(1,15,1), overallAccuracy = rep(0,15))

#Using a for loop try each value of k and add to the data frame the accuracy of correct predictions
#We use the training and validation sets for this purpose so we can keep our test data unseen
for(i in 1:15) {
  
  knn_pred = class::knn(train = trainDF_norm, test = validationDF_norm, cl = trainDF$fuel_type_code_pudl, k = i)
  
  accuracyDF[i, 2] = confusionMatrix(knn_pred, as.factor(validationDF$fuel_type_code_pudl))$overall[1]
  
}
# display the value of k and the overall accuracy
accuracyDF[which(accuracyDF[,2] == max(accuracyDF[,2])),]

trainDF_norm_join = rbind(trainDF_norm, validationDF_norm)
train_labels_join = c(train_labels, validation_labels )

knn_prediction = knn(trainDF_norm_join, testDF_norm, cl=train_labels_join, k = 1)

confusionMatrix(knn_prediction, test_labels)

#Creation of the Naive Bayes Classifier
nb_model = naive_bayes(fuel_type_code_pudl ~fuel_mmbtu_per_unit + sulfur_content_pct + ash_content_pct + fuel_cost_per_mmbtu, data = trainDF, laplace = 0)

#Predictions made using the Classifier
train_pred = predict(nb_model, validationDF)

#confusion matrix
validationDF$fuel_type_code_pudl = as.factor(validationDF$fuel_type_code_pudl)
confusionMatrix(train_pred, validationDF$fuel_type_code_pudl)

trainDF_All = rbind(trainDF, validationDF)

nb_model_all = naive_bayes(fuel_type_code_pudl ~fuel_mmbtu_per_unit + sulfur_content_pct + ash_content_pct + fuel_cost_per_mmbtu, data = trainDF_All, laplace = 0)

#Predictions made using the Classifier
train_pred_All = predict(nb_model_all, testDF)

#confusion matrix
testDF$fuel_type_code_pudl = as.factor(testDF$fuel_type_code_pudl)
confusionMatrix(train_pred_All, testDF$fuel_type_code_pudl)
```
## Executive Summary
This report suggests that Oil may be more efficient then Gas with similar amounts of pollutants but the cost makes it prohibitive to use. Gas is not used as much due to the higher cost and lower output. Coal is the cheapest option with the highest efficiency but the highest in pollutants. These three fuel sources have distinct characteristics which lead to highly accurate clustering and classification.  
The U.S. relies heavily on coal for its energy needs and in this data set did not mention any other types of energy other then fossil fuels. In order to promote the usage of additional energy the costs for oil and gas would need to be a considerate amount less then coal. As energy companies are more concerned with the highest output at the lowest cost instead of what is best for the environment.

## Introduction
After importing the provided data set, the seed value was set to the last 4 digits of my student number. Next, the data structure was looked at to determine the types of variables. There are two variables of type char that will need to be changed to factors if they are to be used. Next, a summary of statistics was performed and showed the potential of outliers as well as how different the scales are between the variables. Next, the data was checked for missing values. No missing values were identified. Dummy Variables were created for the fuel_type_code_pudl as this was a requested value to be used in the clustering analysis. Next, boxplots were created to identity outliers. No outliers were identified in fuel_mbtu_per_unit or ash_content_pct.

```{r echo=FALSE}
boxplot(fuelDF$fuel_received_units, main = "Fuel Recieved Units")
boxplot(fuelDF$sulfur_content_pct, main = "Sulfur Content Pct")
boxplot(fuelDF$fuel_cost_per_mmbtu, main = "Fuel Cost Per MMBTU")
hist(fuelDF$sulfur_content_pct)
```


There are some outliers in fuel_received_units, sulfur_content_pct, and fuel_cost_per_mmbtu. Uppon investigating each variable sulfur_content_pct was kept because it is a percent and makes sense that some types of fuels would have higher sulfur content, this variable is to important to leave out. fuel_cost_per_mmbtu was also left alone as it is a average price and it is not uncommon for outliers to appear in pricing data. Further investigation was done on fuel_received_units, this variable had wide ranges of values and is described in the data as a value associated with either tons, barrels, or Mcf. Having mixed units inside the same variable can lead to issues and lease accuracy. Assuming that each of these units align with a individual energy source summary data was looked at and there still appeared to be wide ranges throughout. At this point the value was kept intake and was discovered in a correlation matrix to have very small correlation that this variable was removed.

```{r echo=FALSE}

ggcorrplot(cor_matrix)

```

The final preprocessing step taken was to remove the char fuel_type_code as this was now captured in 3 dummy variables. Once this value was removed the data was normalized using z-score.

### What could this data be used for?  
This data seems best used for identifying price vs output vs type vs pollution. If the energy plants were left in the data we could use this data to identify biggest polluters and who produces the most energy vs cost/pollution.

## Clustering Analysis and Discussion
Three types of clustering were used to conduct analysis on the data set.  

### K-means Clustering
First, K-Means Clustering was used. Based on the instruction to include the fuel type, I felt that a good value for k would be 3. This also confirmed that the fuel_received_units should be removed from our clustering data as this would be useful if we were trying to answer questions on the energy plants. Why my assumption was to use 3 for *k* WSS and Silhouette tests were performed with silhouette indicating 3 for the value of *k* and WSS showing a value of 2 at the elbow. 2 was not select and 3 was used as the graph indicated there may be multiple values for *k* and a value of 2 could potentially lead to simplistic results that do not provide much information.
```{r echo=FALSE}
fviz_nbclust(fuelDF.clean, kmeans, method = "silhouette")
```

### What benefit is there to including a categorical value in the cluster analysis?  
While the categorical value will help create distinct clusters we may be able to find trends within the clusters themeselves that can prove useful. In order to do this the value of K would need to be increased. However, for K-means 3 clusters was choosen. Additional details on amount of clusters and results are discussed bellow.  

### What distance measure should be used?
The decision was made to use euclidean for distance, Gowers and Jacquard were also considered and investigated. However, the results using these distances/similarities did not appear to provide results that could be interpreted meaningfully. Looking at the clusters generated by K-means we can see 3 distinct clusters and each of them is equal to oil, gas, or coal. By counting the number of each in our data set and comparing this to the size of each cluster there is an exact match. This suggests that K-means may not be as useful for discovering unknown patterns. This also shows that including the categorical variable is dominating the other variables.

```{r echo=FALSE}
fviz_cluster(kclusters, data = fuelDF.clean)
```

### What do each cluster represent?
Looking at the graph of centroids it is determined that Cluster 1 is coal, cluster 3 is gas, and cluster 2 is oil. We can also determine that coal costs the least compared to the amount of energy released but is highest in pollutants of both ash and sulfur. Oil produced the next highest amount of mmbtu but costs the most. Lastly, cluster 3 which is gas has the lowest mmbtu and a cost in the middle. Both cluster 2 and 3 have similar sulfur and ash content.

```{r echo=FALSE}
mincenter = min(kclusters$centers)
maxcenter = max(kclusters$centers)
plot(c(0), xaxt = 'n', ylab = "", type ="l", ylim = c(mincenter, maxcenter), xlim = c(0, 7))
axis(1, at = c(1:7), xpd = TRUE, labels = c("Coal","Gas","Oil","MMBTU", "Sulfur", "Ash", "Cost"), cex.axis = 1)


lines(kclusters$centers[1,], lty = 1, lwd = 2, col = "red")
lines(kclusters$centers[2,], lty = 2, lwd = 2, col = "green")
lines(kclusters$centers[3,], lty = 3, lwd = 2, col = "blue")

legend(x = "topleft", box.col = "brown", 
       bg ="yellow", box.lwd = 2 , title="Clusters",  
       legend=c("Cluster 1", "Cluster 2", "Cluster 3"),  
       fill = c("red","green", "blue")) 

```

### What fuel will provide the least pollution at the lowest cost?  
This can be answered by looking at cluster 2 and 3 which are similar outside of price. This is seen by both clusters being on the left side of the plot. Looking back at the centroid graph it the cheapest option with the least pollution would be Gas(cluster 3), but this also puts out the least amount of energy. However, this would be a good starting point to determine actual pricing and look into additional variables that are not part of the data set. Such as location, time of year, political climate, etc. 

### DBSCAN  
### How will the value for minpts and epsilon be selected?  

Using the same data as K-Means DSCAN was conducted. First, a value for epsilon was determined based of KNN and a value of 5 for *k*. 5 was chosen because of a heuristic rule for selecting min points of (features + 1). I considered there being only 4 features even though we have 7 because the dummyVars all represent the same category. In doing this I selected a value of 0.7 for epsilon as this appears to be the point on the plotted KNN distances to be where distance starts to increase the most. Minpoints is 5 as it should be the same value as what was selected in performing the KNN test.  

```{r echo=FALSE}
dbscan::kNNdistplot(fuelDF.clean, k = 5)
abline(h = 0.7, lty = 2)

```

### Why did DBSCAN create 4 clusters instead of 3?  

The results of the DBSCAN created 4 clusters and some noise points. These 4 clusters appear similar to the results in K-Means. However, the bottom left cluster that we had in K-Means is split into 2 in DBSCAN. This split of the oil cluster suggests that there is enough difference consider two types of oil. While we did not include the plants that produced the energy the data points are from multiple locations this could mean that there are at least 6 observations that are based on more then just them being oil. An additional note which could be slightly seen in k-means is that Cluster 1 in DBSCAN which is assumed to be gas is linear this seems accurate as from the energy industry on downside to using gas all the time is the variability in its costs. By changing the amount of minpoints the amount of clusters can change. If it was important that only 3 clusters where to be identified this could be achieved by increasing the minpoints to 6 but would also identify more noise.

```{r echo=FALSE}
fviz_cluster(dbs, fuelDF.clean, stand = FALSE, geom = "point")
```

### Hierarchical Clustering

### What type of Hierarchical Clustering provided the highest coefficent?  

The third type of clustering conducted was Hierarchical. Both AGNES and DIANA where conducted as well as the different methods to perform AGNES, it was determined to use AGNES with the Ward method as it produced the highest coefficient at 0.99887. 

### Why select 3 clusters instead of what appears to be 5 clusters in the dendogram?  

From the AGNES dendogram and the plotted clusters we can see a larger vertical line for one cluster indicating it has more differences then the others. Next, there is a shorter line that splits into two groups with almost equal length lines indicating that the second split has similar properties. This can be confirmed in the visual plot of the clusters that again is similar to our other clustering methods. However, while 3 is being used for our cut line, investigation was done into the cluster furthest to the right which is assumed to be coal. From the dendogram it appears that there are many more variations and a cut line of 5 was looked into. Using 5 for the number of clusters did highlight a difference but there is much overlap and no clear distinction that could just be attributed to the hierarchy process.

```{r echo=FALSE}
pltree(agnes_ward, cex = 0.6, hang = -1, main = "Dendrogram of Agnes")
rect.hclust(agnes_ward, k = 3, border = 1:3)

agnes.grp = cutree(agnes_ward, k = 3)
fviz_cluster(list(data = fuelDF.clean, cluster = agnes.grp))

```

## Classification Analysis and Discussion
Two types of classification were used in this analysis. First, the data was partitioned into 70% training data, 20% validation data, and 10% test data. Each of these partitions were stratified based on the fuel_type_code_pudl. As with the clustering the fuel_received_units has been left out. Note: The partitioning was completed on non-normalized data and the data was then normalized to conduct KNN and left alone to perform naive Bayes.

### KNN
Using the normalized partitions and the training and validation data a loop was created to try values of *k* 1 - 15 to find the best value of *k* to use. The results showed that values 1,2,3 or 4 would have 100% accuracy. 

### What value of *k* should be used when there is a multi-way tie?  

I decided to select the value of 1 for *k*.I came to this value from not only the test conducted but based on the very distinct clusters that were created in the cluster analysis. Once the value of *k* was determined the testing data and validation data were combined and a model was created to use the test data. A confusion matrix was created which showed 100% accuracy.

```{r echo=FALSE}
print("Confusion Matrix of predictions using model on test data KNN")
confusionMatrix(knn_prediction, test_labels)
```


### Naive Bayes  
Using Naive Bayes a model was created and tested against the validation partition. After viewing the confusion matrix which showed 100% accuracy a new modle was created using both the training and validation partitions and used with the testing data. Another confusion matrix was created indicating a 100% accuracy.

```{r echo=FALSE}
print("Confusion Matrix of predictions using model on test data NB")
confusionMatrix(train_pred_All, testDF$fuel_type_code_pudl)
```


### Comparison
### What does both methods showing 100% accuracy indicate?  

100% accuracy indicates that the selected variables are able to classify an energy source extremely well. I did want to confirm that while I was coming up with 100% accuracy for both models would this always hold true? I removed my seed value and ran the code a few times and did have some instances of less then 100% however they were few.


## Conclusion
In conclusion I am assuming that this data may be more useful in determining associations around the different energy produces instead of the actual energy source. From the results of the clustering and classification it would seem to suggest that telling Oil, Gas, and coal apart is easier to accomplish then other items. I think back to the wine assignments and could see and find small differences that lead to more overlap which makes sense because of the complexities in taste vs the amount of heat and pollutants that are produced from fossil fuels. It would be interesting to see the changes if additional fuels and data were added to the data set.