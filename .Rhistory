b = 1947.01
class(b)
b = 1947.01
class(b)
b = 1947.01
class(b)
b_char = as.character(b)
class(b_char)
#Question 1
b = 1947.01
class(b)
#Question 2
b_char = as.character(b)
class(b_char)
#Question 3
vector = c(1, 'a', 2, 'b')
class(vector)
#Question 1
b = 1947.01
class(b)
#Question 2
b_char = as.character(b)
class(b_char)
#Question 3
vector = c(1, 'a', 2, 'b')
class(vector)
#Question 4
df = data.frame(C1 = c(1,4.37), C2 = c("red", "Blue"), C3 = c(TRUE,FALSE))
View(df)
#Question 1
b = 1947.01
class(b)
#Question 2
b_char = as.character(b)
class(b_char)
#Question 3
vector = c(1, 'a', 2, 'b')
class(vector)
#Question 4
df = data.frame(C1 = c(1,4.37), C2 = c("red", "Blue"), C3 = c(TRUE,FALSE))
#Question 5
print(df$C1)
#Question 1
b = 1947.01
class(b)
#Question 2
b_char = as.character(b)
class(b_char)
#Question 3
vector = c(1, 'a', 2, 'b')
class(vector)
#Question 4
df = data.frame(C1 = c(1,4.37), C2 = c("red", "Blue"), C3 = c(TRUE,FALSE))
#Question 5
print(df$C1)
#Question 6
df2 = data.frame(V1=1:6, Countries=c('US', 'UK','UK', 'India','China','India'))
table(df2)
count(df2$Countries)
library(dplyr)
df2 %>% group_by(Countries) %>% count(Countries)
df2 %>% group_by(Countries)
x = 0.75
if(x >= 0) {
y = 14
} else {
y = -19.7
}
y
x = -1
if(x >= 0) {
y = 14
} else {
y = -19.7
}
y
library(caret)
library(ISLR)
#load ISLR which has the Carseats data set and the caret package
library(ISLR)
library(caret)
#Find out how many observations are in Carseats before splitting
dim(Carseats)
#Create temp_index which will be used to select ~75% or ~300 observations to later create Temp data frame.The selection is stratified by Sales (as in the module 3 example)
temp_index = createDataPartition(Carseats$Sales, p = 0.75, list = FALSE)
#Creation of the Temp Data table which contains ~300 observations to later be split into two data sets
Temp = Carseats[temp_index,]
#Dimensions of Temp
dim(Temp)
#Creation of the Test data set which has ~25% of all the data or ~100 observations
Test = Carseats[-temp_index,]
#Dimensions of Test
dim(Test)
#Create Train_Index which will be used to select ~66% or ~200 observations to later create Train_Data
Train_Index = createDataPartition(Temp$Sales, p = 0.66, list = FALSE)
#Creation of Train_Data set which will have ~200 observations from the original data set
Train_Data = Temp[Train_Index,]
#Dimensions of Train_Data
dim(Train_Data)
#Creation of Validation_Data set which will have ~100 observations form the original data set
Validation_Data = Temp[-Train_Index,]
#Dimensions of Validation_Data
dim(Validation_Data)
#Summary of Train_Data Sales variable
summary(Train_Data$Sales)
#Summary of Validation_Data Sales variable
summary(Validation_Data$Sales)
#Summary of Test data Sales variable
summary(Test$Sales)
#Check for duplicate rows in Test
anyDuplicated(Test)
#Check for duplicate rows in Train_Data
anyDuplicated(Train_Data)
#Check for duplicate rows in Validation_Data
anyDuplicated(Validation_Data)
#Combine all data sets back into one set
all_data = rbind(Test, Train_Data, Validation_Data)
#Check for duplicate rows between all data sets
anyDuplicated(all_data)
knn_Test = class::knn(train = trainDF_norm, test = testDF_norm, cl = trainDF$STATUS, k = 1)
library(caret)
library(class)
#Set seed to have consistent numbers while testing
set.seed(123)
#imported Vehicles_Sales.csv in to a data frame named Vehicles_data
Vehicles_data = read.csv("C:\\Users\\juanm\\OneDrive\\BA 64060 Fundementals of Machine Learning\\Module 4\\Assignment 3\\Vehicles_Sales.csv")
#Dropped YEAR_ID and PRODUCTLINE from the Vehicles_data frame
Vehicles_data = Vehicles_data[,-c(9,10)]
#Converted DEALSIZE to a factor
Vehicles_data$DEALSIZE = as.factor(Vehicles_data$DEALSIZE)
#Created dummyVars for the data
groups = dummyVars(~., data = Vehicles_data)
#applied the dummyvars to the data for use
Vehicles_data = as.data.frame(predict(groups, Vehicles_data))
#Partitioned the data in half and stratified based on the target variable
train_index = createDataPartition(Vehicles_data$STATUS, p = 0.5, list = FALSE)
#Half of the data will be used for training
trainDF = Vehicles_data[train_index,]
#The other half will be further partitioned into validation and testing
tempdf = Vehicles_data[-train_index,]
#Partitioned the remaining data into validation index of 30% stratified based on the target variable to stay consistent
validation_index = createDataPartition(tempdf$STATUS, p = 0.3, list = FALSE)
#Created the validation data frame
validationDF = tempdf[validation_index,]
#Created the testDF using the left over data
testDF = tempdf[-validation_index,]
#Removed the temp data frame
rm(tempdf)
#created normalized values using z-score normalization, removing STATUS from the data first.
norm_values = preProcess(trainDF[,-6], method= c("center","scale"))
#Based on the normalized values from the training set testing, validation, and training normalized data frames are created.
#Making sure STATUS is removed from each before creation.
testDF_norm = predict(norm_values, testDF[,-6])
validationDF_norm = predict(norm_values, validationDF[,-6])
trainDF_norm = predict(norm_values, trainDF[,-6])
#A new data frame containing one observation is created to see if our model can classify it correctly
new_vehicle = data.frame(
ORDERNUMBER = 10322,
QUANTITYORDERED=50,
PRICEEACH=100,
ORDERLINENUMBER=6,
SALES=12536.5,
QTR_ID=4,
MONTH_ID=11,
MSRP=127,
DEALSIZE.Large=1,
DEALSIZE.Small=0,
DEALSIZE.Medium=0
)
#A new variable is created so we are not manipulating the original
new_vehicle_norm = new_vehicle
#normalizing or new vehicle that we want to classify
new_vehicle_norm = predict(norm_values, new_vehicle_norm)
#Predicting the nearest neighbor
knn_pred_k_1 = class::knn(train = trainDF_norm, test = new_vehicle_norm, cl = trainDF$STATUS, k = 1)
#The Nearest Neighbor
knn_pred_k_1
knn_Test = class::knn(train = trainDF_norm, test = testDF_norm, cl = trainDF$STATUS, k = 1)
#Created a data frame that contains values of k from 1 - 15
accuracyDF = data.frame(k = seq(1,15,1), overallAccuracy = rep(0,15))
#Using a for loop try each value of k and add to the data frame the accuracy of correct predictions
#We use the training and validation sets for this purpose so we can keep our test data unseen
for(i in 1:15) {
knn_pred = class::knn(train = trainDF_norm, test = validationDF_norm, cl = trainDF$STATUS, k = i)
accuracyDF[i, 2] = confusionMatrix(knn_pred, as.factor(validationDF$STATUS), positive = "1")$overall[1]
}
# display the value of k and the overall accuracy
accuracyDF[which(accuracyDF[,2] == max(accuracyDF[,2])),]
knn_Test = class::knn(train = trainDF_norm, test = testDF_norm, cl = trainDF$STATUS, k = 1)
knn_Test
knn_Test = class::knn(train = trainDF_norm, test = testDF_norm, cl = trainDF$STATUS, k = 1)
knn_Test
confusionMatrix(knn_Test, as.factor(testDF$STATUS))
knn_Test = class::knn(train = trainDF_norm, test = testDF_norm, cl = trainDF$STATUS, k = 1)
knn_Test
confusionMatrix(knn_Test, as.factor(testDF$STATUS), positive = "1")$overall[1]
knn_Test = class::knn(train = trainDF_norm, test = testDF_norm, cl = trainDF$STATUS, k = 1)
knn_Test
confusionMatrix(knn_Test, as.factor(testDF$STATUS), positive = "1")
